{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g6ANDtpv7X8"
      },
      "source": [
        "#**ANÁLISE EVOLUTIVA DA INTERCONEXÃO DAS LINGUAGENS DE PROGRAMAÇÃO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWEQstA34N1j"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas Utilizadas Para Execução do Código\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTFY6xuwfRX"
      },
      "source": [
        "# **Tratamento da base de dados**\n",
        "\n",
        "> Primeiramente, foi-se extraído do site www.kaggle.com uma base de dados dos repositórios do GitHub, dos anos de 2008 à 2023. Tais repositórios apresentam diversas características:\n",
        "\n",
        "* repo_id                     \n",
        "* owner_username\n",
        "* repo_name\n",
        "* description\n",
        "* created_at\n",
        "* pushed_at\n",
        "* size\n",
        "* stargazers_count\n",
        "* has_projects\n",
        "* has_wiki\n",
        "* has_pages\n",
        "* forks_count\n",
        "* open_issues_count\n",
        "* license\n",
        "* is_template\n",
        "* topics\n",
        "* watching\n",
        "* contributors_count\n",
        "* commits_count\n",
        "* languages\n",
        "* readme\n",
        "\n",
        "Para esse trabalho, a coluna *languagens* foi a escolhida como extremamente relevante, tendo em vista que a interconexão das linguagens se dá quando elas são usadas no mesmo repositório em questão.\n",
        "\n",
        "[Repositories.csv](https://www.kaggle.com/datasets/aditijuneja/github-bipartite-graph-datasetdevelopersrepos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utiWUaSMfRE6"
      },
      "outputs": [],
      "source": [
        "#Passo 1: Remover as colunas que não são importantes para esse trabalho\n",
        "# Carregar o dataframe\n",
        "df_repositories = pd.read_csv('/content/repositories.csv')\n",
        "\n",
        "# Remover colunas no dataframe df_repositories\n",
        "colunas_a_remover_repositories = ['repo_id', 'pushed_at', 'size', 'has_projects', 'has_wiki',\n",
        "                                  'has_pages', 'license', 'is_template', 'topics', 'watching', 'commits_count', 'readme']\n",
        "df_repositories = df_repositories.drop(columns=colunas_a_remover_repositories, errors='ignore')\n",
        "\n",
        "# Salvar o dataframe modificado\n",
        "df_repositories.to_csv('/content/repositories_etapa_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfECT-d0RsOU"
      },
      "outputs": [],
      "source": [
        "#Passo 2: Tratar a data no dataframe repositories.csv\n",
        "# Carregar o CSV para um DataFrame do Pandas\n",
        "df_repositories_etapa_1 = pd.read_csv('/content/repositories_etapa_1.csv')\n",
        "\n",
        "# Função para extrair o ano de uma string de data\n",
        "def extrair_ano(data):\n",
        "    data_objeto = datetime.strptime(data, '%Y-%m-%d %H:%M:%S')\n",
        "    return str(data_objeto.year)\n",
        "\n",
        "# Aplicar a função à coluna 'created_at'\n",
        "df_repositories_etapa_1['created_at'] = df_repositories_etapa_1['created_at'].apply(extrair_ano)\n",
        "\n",
        "# Salvar o DataFrame modificado de volta ao CSV\n",
        "df_repositories_etapa_1.to_csv('/content/repositories_etapa_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Z5sGW-Smv3"
      },
      "outputs": [],
      "source": [
        "# Passo 3: Tratar as linguagens vazias no dataframe repositories.csv\n",
        "# Carregar o dataframe do arquivo CSV\n",
        "df_repositories_etapa_2 = pd.read_csv('/content/repositories_etapa_2.csv')\n",
        "\n",
        "# Excluir linhas onde a coluna 'languages' é igual a '[]'\n",
        "df_repositories_etapa_2 = df_repositories_etapa_2[df_repositories_etapa_2['languages'] != '[]']\n",
        "\n",
        "# Salvar o dataframe modificado\n",
        "df_repositories_etapa_2.to_csv('/content/repositories_etapa_3.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7oF7Yi_Vvt2"
      },
      "outputs": [],
      "source": [
        "# Passo 6: Tirar os repositórios que só tem uma linguagem (Interconexção)\n",
        "# Carregar o dataframe do arquivo CSV\n",
        "df_repositories = pd.read_csv('/content/repositories_etapa_3.csv')\n",
        "\n",
        "# Excluir os repositórios que só tem uma linguagem\n",
        "df_repositories = df_repositories[df_repositories['languages'].apply(lambda x: len(eval(x)) > 1)]\n",
        "\n",
        "#Salvar o dataframe modificado\n",
        "df_repositories.to_csv('/content/repositories_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVrhhzCkZ03F"
      },
      "outputs": [],
      "source": [
        "# Extrair a base de 2010\n",
        "# Extração das bases de 2010 - 2015 - 2020\n",
        "\n",
        "# Carregar o dataframe repositories tratado\n",
        "df_repositories = pd.read_csv('/content/repositories_final.csv', parse_dates=['created_at'])\n",
        "\n",
        "# Filtrar as linhas onde 'created_at' começa com o ano desejado\n",
        "df_repositories_2020 = df_repositories[df_repositories['created_at'].dt.year == 2020]\n",
        "\n",
        "# Salvando o DataFrame resultante em um novo arquivo CSV\n",
        "df_repositories_2020.to_csv('/content/repositories_2020.csv', index=False, date_format='%Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzDaY2TmFLQv"
      },
      "source": [
        "# Construção do **Grafo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUZlLJ5G35gT"
      },
      "outputs": [],
      "source": [
        "# Construção do Grafo (2010 - 2015 - 2020)\n",
        "\n",
        "# Carregando o arquivo CSV\n",
        "df_repositories_2020 = pd.read_csv('/content/repositories_2020.csv')\n",
        "\n",
        "# Extraindo a coluna de linguagens do DataFrame\n",
        "df_repositories_2020_languages = df_repositories_2020['languages']\n",
        "\n",
        "# Inicializando um grafo não direcionado usando NetworkX\n",
        "cont = 0\n",
        "H = nx.Graph()\n",
        "mapa = dict()\n",
        "\n",
        "# Iterando sobre as strings na coluna 'languages'\n",
        "for row in df_repositories_2020_languages:\n",
        "    languages_list = [lang.strip(\"'[] \") for lang in row.split(',')]\n",
        "    aux = languages_list[0]\n",
        "    cont = 0\n",
        "    for _ in languages_list:\n",
        "      aux = languages_list[cont]\n",
        "      cont += 1\n",
        "      for lang in languages_list:\n",
        "        if aux != lang:\n",
        "          if aux not in mapa:\n",
        "              mapa[aux] = {lang: 1}\n",
        "          else:\n",
        "              if lang in mapa[aux]:\n",
        "                  mapa[aux][lang] += 1\n",
        "              else:\n",
        "                  mapa[aux][lang] = 1\n",
        "\n",
        "# Adicionando arestas ponderadas ao grafo\n",
        "for No_1, list_adjacent in mapa.items():\n",
        "  for No_2, Peso in list_adjacent.items():\n",
        "    H.add_edge(No_1, No_2, weight = Peso)\n",
        "\n",
        "# Configurando a visualização do grafo com Kamada-Kawai\n",
        "fig, ax = plt.subplots(figsize=(25, 20))\n",
        "edge_map = dict(zip(H.edges(), range(len(H.edges()))))\n",
        "edge_colors = [\"black\"] * len(H.edges())\n",
        "line_width = [1] * len(H.edges())\n",
        "node_size_factor = 50\n",
        "node_size = [node_size_factor * H.degree[node] for node in H.nodes()]\n",
        "nx.draw_kamada_kawai(\n",
        "    H, with_labels=True, font_weight=\"bold\", edge_color=edge_colors, width=line_width, node_size=node_size\n",
        ")\n",
        "plt.show()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construção da **Árvore Geradora Máxima**"
      ],
      "metadata": {
        "id": "VWHpSJAJaJ_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo a árvore de extensão máxima\n",
        "maximo = nx.maximum_spanning_tree(H)\n",
        "new_Graph = nx.Graph()\n",
        "for edge in maximo.edges(data=True):\n",
        "  if edge[2]['weight'] > 5:\n",
        "    new_Graph.add_edge(edge[0], edge[1], weight=edge[2]['weight'])\n",
        "\n",
        "# Configurando a visualização do grafo com Kamada-Kawai\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "pos = nx.spring_layout(new_Graph)\n",
        "nx.draw(new_Graph, pos, with_labels=True, font_weight=\"bold\")\n",
        "edge_labels = nx.get_edge_attributes(new_Graph, 'weight')\n",
        "nx.draw_networkx_edge_labels(new_Graph, pos, edge_labels=edge_labels, font_color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9PdnbANoaPIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}